{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_samples</th>\n      <th>n_features</th>\n      <th>n_informative</th>\n      <th>n_redundant</th>\n      <th>n_repeated</th>\n      <th>n_classes</th>\n      <th>n_clusters_per_class</th>\n      <th>weights</th>\n      <th>flip_y</th>\n      <th>class_sep</th>\n      <th>...</th>\n      <th>amount_of_lastly_al_samples</th>\n      <th>JOB_ID</th>\n      <th>AL_SAMPLES_WEIGHT</th>\n      <th>AMOUNT_OF_LFS</th>\n      <th>DATASET</th>\n      <th>DATASET_RANDOM_GENERATION_SEED</th>\n      <th>FRACTION_OF_INITIALLY_LABELLED_SAMPLES</th>\n      <th>FRACTION_OF_LASTLY_AL_LABELLED_SAMPLES</th>\n      <th>LF_RANDOM_SEED</th>\n      <th>MERGE_WS_SAMPLES_STRATEGY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>351</td>\n      <td>35</td>\n      <td>35</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>...</td>\n      <td>111</td>\n      <td>90</td>\n      <td>99</td>\n      <td>1.360981</td>\n      <td>IONOSPHERE</td>\n      <td>958448</td>\n      <td>0.075359</td>\n      <td>0.691714</td>\n      <td>715461</td>\n      <td>RandomLabelMergeStrategy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>768</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>...</td>\n      <td>37</td>\n      <td>86</td>\n      <td>73</td>\n      <td>2.039518</td>\n      <td>DIABETES</td>\n      <td>690908</td>\n      <td>0.270958</td>\n      <td>0.133185</td>\n      <td>122403</td>\n      <td>MajorityVoteLabelMergeStrategy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>182</td>\n      <td>13</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>...</td>\n      <td>19</td>\n      <td>122</td>\n      <td>73</td>\n      <td>5.757419</td>\n      <td>PLANNING</td>\n      <td>93071</td>\n      <td>0.471576</td>\n      <td>0.411841</td>\n      <td>42079</td>\n      <td>MajorityVoteLabelMergeStrategy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>569</td>\n      <td>32</td>\n      <td>32</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>...</td>\n      <td>5</td>\n      <td>118</td>\n      <td>51</td>\n      <td>6.895767</td>\n      <td>BREAST</td>\n      <td>137542</td>\n      <td>0.967994</td>\n      <td>0.710952</td>\n      <td>189140</td>\n      <td>MajorityVoteLabelMergeStrategy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>768</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>...</td>\n      <td>74</td>\n      <td>125</td>\n      <td>14</td>\n      <td>6.302852</td>\n      <td>PIMA</td>\n      <td>873173</td>\n      <td>0.186405</td>\n      <td>0.235692</td>\n      <td>917582</td>\n      <td>SnorkelLabelMergeStrategy</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8456</th>\n      <td>10000</td>\n      <td>3073</td>\n      <td>3073</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>...</td>\n      <td>826</td>\n      <td>9821</td>\n      <td>33</td>\n      <td>5.283030</td>\n      <td>cifar10</td>\n      <td>133685</td>\n      <td>0.751403</td>\n      <td>0.670315</td>\n      <td>851257</td>\n      <td>MajorityVoteLabelMergeStrategy</td>\n    </tr>\n    <tr>\n      <th>8457</th>\n      <td>10000</td>\n      <td>3073</td>\n      <td>3073</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>...</td>\n      <td>352</td>\n      <td>9906</td>\n      <td>24</td>\n      <td>1.192563</td>\n      <td>cifar10</td>\n      <td>65048</td>\n      <td>0.738701</td>\n      <td>0.271355</td>\n      <td>143076</td>\n      <td>MajorityVoteLabelMergeStrategy</td>\n    </tr>\n    <tr>\n      <th>8458</th>\n      <td>10000</td>\n      <td>3073</td>\n      <td>3073</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>...</td>\n      <td>2769</td>\n      <td>9882</td>\n      <td>43</td>\n      <td>9.592007</td>\n      <td>cifar10</td>\n      <td>349941</td>\n      <td>0.413283</td>\n      <td>0.947198</td>\n      <td>102159</td>\n      <td>RandomLabelMergeStrategy</td>\n    </tr>\n    <tr>\n      <th>8459</th>\n      <td>10000</td>\n      <td>3073</td>\n      <td>3073</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>...</td>\n      <td>618</td>\n      <td>9994</td>\n      <td>44</td>\n      <td>1.232178</td>\n      <td>cifar10</td>\n      <td>663055</td>\n      <td>0.774100</td>\n      <td>0.552117</td>\n      <td>975948</td>\n      <td>RandomLabelMergeStrategy</td>\n    </tr>\n    <tr>\n      <th>8460</th>\n      <td>10000</td>\n      <td>3073</td>\n      <td>3073</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>?</td>\n      <td>...</td>\n      <td>8</td>\n      <td>9918</td>\n      <td>69</td>\n      <td>5.282065</td>\n      <td>cifar10</td>\n      <td>635764</td>\n      <td>0.961230</td>\n      <td>0.040436</td>\n      <td>247303</td>\n      <td>MajorityVoteLabelMergeStrategy</td>\n    </tr>\n  </tbody>\n</table>\n<p>8461 rows × 57 columns</p>\n</div>",
      "text/plain": "      n_samples  n_features  n_informative  n_redundant  n_repeated  \\\n0           351          35             35            0           0   \n1           768           9              9            0           0   \n2           182          13             13            0           0   \n3           569          32             32            0           0   \n4           768           9              9            0           0   \n...         ...         ...            ...          ...         ...   \n8456      10000        3073           3073            0           0   \n8457      10000        3073           3073            0           0   \n8458      10000        3073           3073            0           0   \n8459      10000        3073           3073            0           0   \n8460      10000        3073           3073            0           0   \n\n      n_classes n_clusters_per_class weights flip_y class_sep  ...  \\\n0             2                    ?       ?      ?         ?  ...   \n1             2                    ?       ?      ?         ?  ...   \n2             2                    ?       ?      ?         ?  ...   \n3             2                    ?       ?      ?         ?  ...   \n4             2                    ?       ?      ?         ?  ...   \n...         ...                  ...     ...    ...       ...  ...   \n8456         10                    ?       ?      ?         ?  ...   \n8457         10                    ?       ?      ?         ?  ...   \n8458         10                    ?       ?      ?         ?  ...   \n8459         10                    ?       ?      ?         ?  ...   \n8460         10                    ?       ?      ?         ?  ...   \n\n     amount_of_lastly_al_samples JOB_ID  AL_SAMPLES_WEIGHT AMOUNT_OF_LFS  \\\n0                            111     90                 99      1.360981   \n1                             37     86                 73      2.039518   \n2                             19    122                 73      5.757419   \n3                              5    118                 51      6.895767   \n4                             74    125                 14      6.302852   \n...                          ...    ...                ...           ...   \n8456                         826   9821                 33      5.283030   \n8457                         352   9906                 24      1.192563   \n8458                        2769   9882                 43      9.592007   \n8459                         618   9994                 44      1.232178   \n8460                           8   9918                 69      5.282065   \n\n         DATASET DATASET_RANDOM_GENERATION_SEED  \\\n0     IONOSPHERE                         958448   \n1       DIABETES                         690908   \n2       PLANNING                          93071   \n3         BREAST                         137542   \n4           PIMA                         873173   \n...          ...                            ...   \n8456     cifar10                         133685   \n8457     cifar10                          65048   \n8458     cifar10                         349941   \n8459     cifar10                         663055   \n8460     cifar10                         635764   \n\n     FRACTION_OF_INITIALLY_LABELLED_SAMPLES  \\\n0                                  0.075359   \n1                                  0.270958   \n2                                  0.471576   \n3                                  0.967994   \n4                                  0.186405   \n...                                     ...   \n8456                               0.751403   \n8457                               0.738701   \n8458                               0.413283   \n8459                               0.774100   \n8460                               0.961230   \n\n     FRACTION_OF_LASTLY_AL_LABELLED_SAMPLES  LF_RANDOM_SEED  \\\n0                                  0.691714          715461   \n1                                  0.133185          122403   \n2                                  0.411841           42079   \n3                                  0.710952          189140   \n4                                  0.235692          917582   \n...                                     ...             ...   \n8456                               0.670315          851257   \n8457                               0.271355          143076   \n8458                               0.947198          102159   \n8459                               0.552117          975948   \n8460                               0.040436          247303   \n\n           MERGE_WS_SAMPLES_STRATEGY  \n0           RandomLabelMergeStrategy  \n1     MajorityVoteLabelMergeStrategy  \n2     MajorityVoteLabelMergeStrategy  \n3     MajorityVoteLabelMergeStrategy  \n4          SnorkelLabelMergeStrategy  \n...                              ...  \n8456  MajorityVoteLabelMergeStrategy  \n8457  MajorityVoteLabelMergeStrategy  \n8458        RandomLabelMergeStrategy  \n8459        RandomLabelMergeStrategy  \n8460  MajorityVoteLabelMergeStrategy  \n\n[8461 rows x 57 columns]"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from active_learning.learner.standard import Learner, get_classifier\n",
    "from active_learning.weak_supervision.SelfTraining import SelfTraining\n",
    "import argparse\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from timeit import default_timer as timer\n",
    "from typing import List, Dict\n",
    "from active_learning.config import get_active_config\n",
    "from active_learning.dataStorage import DataStorage\n",
    "from active_learning.datasets import load_synthetic\n",
    "from active_learning.logger import init_logger\n",
    "from active_learning.merge_weak_supervision_label_strategies.MajorityVoteLabelMergeStrategy import (\n",
    "    MajorityVoteLabelMergeStrategy,\n",
    ")\n",
    "from collections import Counter\n",
    "\n",
    "from active_learning.weak_supervision import SyntheticLabelingFunctions\n",
    "from active_learning.weak_supervision.BaseWeakSupervision import BaseWeakSupervision\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "font_size = 8\n",
    "\n",
    "tex_fonts = {\n",
    "    # Use LaTeX to write all text\n",
    "    # \"text.usetex\": True,\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": [\"Times New Roman\", 'sans-serif'],\n",
    "    # Use 10pt font in plots, to match 10pt font in document\n",
    "    \"axes.labelsize\": font_size,\n",
    "    \"font.size\": font_size,\n",
    "    # Make the legend/label fonts a little smaller\n",
    "    \"legend.fontsize\": font_size,\n",
    "    \"xtick.labelsize\": font_size,\n",
    "    \"ytick.labelsize\": font_size,\n",
    "    \"xtick.bottom\": True,\n",
    "    \"figure.autolayout\": True,\n",
    "}\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "plt.rcParams.update(tex_fonts)  # type: ignore\n",
    "\n",
    "\n",
    "# https://jwalton.info/Embed-Publication-Matplotlib-Latex/\n",
    "def set_matplotlib_size(width, fraction=1):\n",
    "    \"\"\"Set figure dimensions to avoid scaling in LaTeX.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    width: float\n",
    "            Document textwidth or columnwidth in pts\n",
    "    fraction: float, optional\n",
    "            Fraction of the width which you wish the figure to occupy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig_dim: tuple\n",
    "            Dimensions of figure in inches\n",
    "    \"\"\"\n",
    "    # Width of figure (in pts)\n",
    "    fig_width_pt = width * fraction\n",
    "\n",
    "    # Convert from pt to inches\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    # Golden ratio to set aesthetic figure height\n",
    "    # https://disq.us/p/2940ij3\n",
    "    golden_ratio = (5 ** 0.5 - 1) / 2\n",
    "\n",
    "    # Figure width in inches\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    # Figure height in inches\n",
    "    fig_height_in = fig_width_in * golden_ratio\n",
    "\n",
    "    fig_dim = (fig_width_in, fig_height_in)\n",
    "\n",
    "    return fig_dim\n",
    "\n",
    "\n",
    "#width = 505.89\n",
    "width = 1500\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"~/exp_results/run_how_to/exp_results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['n_samples', 'n_features', 'n_informative', 'n_redundant', 'n_repeated',\n       'n_classes', 'n_clusters_per_class', 'weights', 'flip_y', 'class_sep',\n       'hypercube', 'scale', 'random_state', 'ABSTAIN_THRESHOLDS',\n       'LF_CLASSIFIERS', 'AMOUNT_OF_LF_FEATURESSS', 'acc_WS', 'f1_WS',\n       'f1_initial', 'acc_initial', 'f1_ws', 'acc_ws',\n       'acc_ws_and_al_UncertaintyMaxMargin_no_ws',\n       'f1_ws_and_al_UncertaintyMaxMargin_no_ws',\n       'acc_al_and_al_UncertaintyMaxMargin_no_ws',\n       'f1_al_and_al_UncertaintyMaxMargin_no_ws',\n       'acc_ws_and_al_UncertaintyMaxMargin_with_ws',\n       'f1_ws_and_al_UncertaintyMaxMargin_with_ws',\n       'acc_al_and_al_UncertaintyMaxMargin_with_ws',\n       'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random',\n       'f1_ws_and_al_Random', 'acc_al_and_al_Random', 'f1_al_and_al_Random',\n       'acc_ws_and_al_CoveredByLeastAmountOfLf',\n       'f1_ws_and_al_CoveredByLeastAmountOfLf',\n       'acc_al_and_al_CoveredByLeastAmountOfLf',\n       'f1_al_and_al_CoveredByLeastAmountOfLf',\n       'acc_ws_and_al_ClassificationIsMostWrong',\n       'f1_ws_and_al_ClassificationIsMostWrong',\n       'acc_al_and_al_ClassificationIsMostWrong',\n       'f1_al_and_al_ClassificationIsMostWrong',\n       'acc_ws_and_al_GreatestDisagreement',\n       'f1_ws_and_al_GreatestDisagreement',\n       'acc_al_and_al_GreatestDisagreement',\n       'f1_al_and_al_GreatestDisagreement', 'amount_of_initial_al_samples',\n       'amount_of_lastly_al_samples', 'JOB_ID', 'AL_SAMPLES_WEIGHT',\n       'AMOUNT_OF_LFS', 'DATASET', 'DATASET_RANDOM_GENERATION_SEED',\n       'FRACTION_OF_INITIALLY_LABELLED_SAMPLES',\n       'FRACTION_OF_LASTLY_AL_LABELLED_SAMPLES', 'LF_RANDOM_SEED',\n       'MERGE_WS_SAMPLES_STRATEGY'],\n      dtype='object')"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Improvement after AL + WS acc\n",
      "                                                               Mean     \n",
      "acc_initial                                        58.09% ±0.48%\n",
      "acc_ws                                             56.03% ±0.47%\n",
      "acc_ws_and_al_UncertaintyMaxMargin_no_ws           57.72% ±0.47%\n",
      "acc_al_and_al_UncertaintyMaxMargin_no_ws           58.64% ±0.48%\n",
      "acc_ws_and_al_UncertaintyMaxMargin_with_ws         57.72% ±0.47%\n",
      "acc_al_and_al_UncertaintyMaxMargin_with_ws         58.64% ±0.48%\n",
      "acc_ws_and_al_Random                               57.60% ±0.46%\n",
      "acc_al_and_al_Random                               58.24% ±0.49%\n",
      "acc_ws_and_al_CoveredByLeastAmountOfLf             57.20% ±0.47%\n",
      "acc_al_and_al_CoveredByLeastAmountOfLf             58.17% ±0.49%\n",
      "acc_ws_and_al_ClassificationIsMostWrong            57.02% ±0.46%\n",
      "acc_al_and_al_ClassificationIsMostWrong            57.52% ±0.48%\n",
      "acc_ws_and_al_GreatestDisagreement                 57.16% ±0.46%\n",
      "acc_al_and_al_GreatestDisagreement                 58.19% ±0.48%\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "General Improvement after AL + WS f1\n",
      "                                                               Mean     \n",
      "f1_initial                                         54.14% ±0.49%\n",
      "f1_ws                                              50.51% ±0.53%\n",
      "f1_ws_and_al_UncertaintyMaxMargin_no_ws            52.90% ±0.52%\n",
      "f1_al_and_al_UncertaintyMaxMargin_no_ws            55.25% ±0.49%\n",
      "f1_ws_and_al_UncertaintyMaxMargin_with_ws          52.90% ±0.52%\n",
      "f1_al_and_al_UncertaintyMaxMargin_with_ws          55.25% ±0.49%\n",
      "f1_ws_and_al_Random                                52.81% ±0.50%\n",
      "f1_al_and_al_Random                                54.80% ±0.48%\n",
      "f1_ws_and_al_CoveredByLeastAmountOfLf              52.36% ±0.50%\n",
      "f1_al_and_al_CoveredByLeastAmountOfLf              54.49% ±0.49%\n",
      "f1_ws_and_al_ClassificationIsMostWrong             51.96% ±0.51%\n",
      "f1_al_and_al_ClassificationIsMostWrong             53.29% ±0.50%\n",
      "f1_ws_and_al_GreatestDisagreement                  52.26% ±0.50%\n",
      "f1_al_and_al_GreatestDisagreement                  54.31% ±0.48%\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n"
     ]
    }
   ],
   "source": [
    "def hist_plot(df: pd.DataFrame, COMPARE_KEYS:List[str], TITLE:str=None, OUTPUT_PATH: str=None, LABEL:str=\"AUC-F1 (\\\\%)\"):\n",
    "    fig = plt.figure(figsize=set_matplotlib_size(width, fraction=0.5))\n",
    "\n",
    "    # fig = plt.gcf()  # type: ignore\n",
    "    # fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "    selection_list = []\n",
    "    for key in COMPARE_KEYS:\n",
    "        selection_list.append((df[key], key))\n",
    "    #for strategy in df[\"strategy\"].unique():\n",
    "    #    selection_list.append((df[df[\"strategy\"] == strategy][\"f1_auc\"], strategy))\n",
    "\n",
    "    for selection, label in sorted(selection_list, key=lambda tup: tup[1]):\n",
    "        #  ax = sns.distplot(selection, label=label, **kwargs)\n",
    "        #  label = \"{}-{:>3}: {:.4g}% \".format(label[0], label[1], selection.mean() * 100)\n",
    "        label = \"{:<10}{:>5.4g}% \".format(label + \":\", selection.mean() * 100)\n",
    "        ax = sns.kdeplot(selection, label=label)\n",
    "        # ax = sns.distplot(selection, label=label, kde=True)\n",
    "        #ax.set_xlim(0, 1)\n",
    "        ax.set_xlim(0.5, 0.6)\n",
    "        #  ax.set_xlim(0.75, 0.8)\n",
    "        #  ax.set_xlim(80, 875)\n",
    "\n",
    "        mean = selection.mean()\n",
    "        if selection.count() == 0:\n",
    "            low = high = mean\n",
    "        else:\n",
    "            low = selection.mean() - 1.96 * selection.std() / math.sqrt(\n",
    "                selection.count()\n",
    "            )\n",
    "            high = selection.mean() + 1.96 * selection.std() / math.sqrt(\n",
    "                selection.count()\n",
    "            )\n",
    "        print(label)\n",
    "        ax.axvline(mean, color=plt.gca().lines[-1].get_color())  # type: ignore\n",
    "        ax.axvspan(low, high, alpha=0.2, color=plt.gca().lines[-1].get_color())  # type: ignore\n",
    "\n",
    "        ax.set_xticks([x for x in ax.get_xticks()]) # stupid \"workaround\" for https://github.com/matplotlib/matplotlib/issues/18848\n",
    "        ax.set_xticklabels([\"{:.0%}\\\\%\".format(x) for x in ax.get_xticks()])\n",
    "    ax.set_title(TITLE)  # type: ignore\n",
    "    # ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(LABEL)  # type: ignore\n",
    "    ax.legend(  # type: ignore\n",
    "        loc=\"center left\"\n",
    "    )  # , bbox_to_anchor=(0.0, 1.01), borderaxespad=0, frameon=False, ncol=len(selection))  # type: ignore\n",
    "    # ax.xa\n",
    "\n",
    "def comparison_table(df: pd.DataFrame, COMPARE_KEYS:List[str], TITLE:str=\"\"):\n",
    "    results = []\n",
    "    for key in COMPARE_KEYS:\n",
    "        selection = df[key]\n",
    "\n",
    "        mean = selection.mean()\n",
    "        error_area = 1.96 * selection.std() / math.sqrt(\n",
    "                selection.count()\n",
    "            )\n",
    "        results.append([key, mean, error_area])\n",
    "    print(TITLE)\n",
    "    print(\"{:>60} {:>6} {:>4}\".format(\"\", \"Mean\", \"\"))\n",
    "    for result in results:\n",
    "        print(\"{:<50} {:>6.2%} ±{:>4.2%}\".format(result[0], float(result[1]), float(result[2])))\n",
    "\n",
    "def answer_questions(df: pd.DataFrame, COMPARE_KEYS:List[str], metric):\n",
    "    results = {}\n",
    "    for key in COMPARE_KEYS:\n",
    "        selection = df[key]\n",
    "\n",
    "        mean = selection.mean()\n",
    "        error_area = 1.96 * selection.std() / math.sqrt(\n",
    "                selection.count()\n",
    "            )\n",
    "        results[key] = (mean, error_area)\n",
    "    \n",
    "    better_than_initial = [k for k,v in results.items() if v[0] > results[metric + \"_initial\"][0]]\n",
    "    better_than_initial_with_ws = [k for k in better_than_initial if \"ws_and_al\" in k]\n",
    "    print(\"Which are better than initial?\", better_than_initial)\n",
    "    print(\"Which are better than initial, with WS?\", better_than_initial_with_ws)\n",
    "\n",
    "    al_ws_al_better_than_al_al = []\n",
    "\n",
    "    for k in results.keys():\n",
    "        if k.startswith(metric + \"_ws_and_al\"):\n",
    "            if results[k][0] > results[k.replace(\"_ws_and_al\", \"_al_and_al\")][0]:\n",
    "                al_ws_al_better_than_al_al.append(k)\n",
    "\n",
    "    print(\"Where is al+ws+al better than al+al?\", al_ws_al_better_than_al_al)\n",
    "    \n",
    "    #print(\"Where is the mean improvement within the 95% error range?\")\n",
    "    \n",
    "    \n",
    "# 01 general improvement\n",
    "for metric in ['acc', 'f1']:\n",
    "    COMPARE_KEYS = [metric+\"_initial\", metric +\"_ws\"]\n",
    "    for AL_SAMPLING_STRATEGY in [\"UncertaintyMaxMargin_no_ws\",\n",
    "            \"UncertaintyMaxMargin_with_ws\",\n",
    "            \"Random\",\n",
    "            \"CoveredByLeastAmountOfLf\",\n",
    "            \"ClassificationIsMostWrong\",\n",
    "            \"GreatestDisagreement\"]:\n",
    "            COMPARE_KEYS.append(metric + '_ws_and_al_'+AL_SAMPLING_STRATEGY)\n",
    "            COMPARE_KEYS.append(metric + '_al_and_al_'+AL_SAMPLING_STRATEGY)\n",
    "    comparison_table(df, COMPARE_KEYS=COMPARE_KEYS, TITLE=\"General Improvement after AL + WS \" + metric)\n",
    "    answer_questions(df, COMPARE_KEYS=COMPARE_KEYS, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "DATASET\n",
      "################################################################################\n",
      "\n",
      "IONOSPHERE :  241\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "DIABETES :  276\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "PLANNING :  254\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "BREAST :  287\n",
      "Which are better than initial? ['acc_ws', 'acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_al_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Which are better than initial? ['f1_ws', 'f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_al_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_GreatestDisagreement']\n",
      "\n",
      "PIMA :  275\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "HABERMAN :  310\n",
      "Which are better than initial? []\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong']\n",
      "Which are better than initial? ['f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "synthetic :  2808\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "HEART :  285\n",
      "Which are better than initial? ['acc_ws', 'acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Which are better than initial? ['f1_ws', 'f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement']\n",
      "\n",
      "zoo :  250\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "GERMAN :  285\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "ILPD :  288\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "flag :  257\n",
      "Which are better than initial? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_al_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_al_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "abalone :  262\n",
      "Which are better than initial? ['acc_ws', 'acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_al_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "FERTILITY :  258\n",
      "Which are better than initial? ['acc_ws', 'acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_al_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_ws', 'f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_al_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "olivetti :  195\n",
      "Which are better than initial? ['acc_ws', 'acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Which are better than initial? ['f1_ws', 'f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_al_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement']\n",
      "\n",
      "adult :  271\n",
      "Which are better than initial? []\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Which are better than initial? []\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement']\n",
      "\n",
      "australian :  302\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "dwtc :  289\n",
      "Which are better than initial? ['acc_ws', 'acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_al_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "parkinsons :  261\n",
      "Which are better than initial? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_al_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "cifar10 :  271\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "glass :  263\n",
      "Which are better than initial? ['acc_ws', 'acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_al_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Where is al+ws+al better than al+al? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "wine :  273\n",
      "Which are better than initial? []\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? ['acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement']\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "################################################################################\n",
      "MERGE_WS_SAMPLES_STRATEGY\n",
      "################################################################################\n",
      "\n",
      "RandomLabelMergeStrategy :  3157\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "MajorityVoteLabelMergeStrategy :  3103\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "\n",
      "SnorkelLabelMergeStrategy :  2201\n",
      "Which are better than initial? ['acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n",
      "Which are better than initial? ['f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_GreatestDisagreement']\n",
      "Which are better than initial, with WS? []\n",
      "Where is al+ws+al better than al+al? []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# group by stuff\n",
    "for group_by in ['DATASET', 'MERGE_WS_SAMPLES_STRATEGY']:\n",
    "    print()\n",
    "    print(\"#\"*80)\n",
    "    print(group_by)\n",
    "    print(\"#\"*80)\n",
    "    for group_by_value in df[group_by].unique():\n",
    "        print()\n",
    "        print(group_by_value, \": \", len(df[df[group_by] == group_by_value]))\n",
    "        for metric in ['acc', 'f1']:\n",
    "            COMPARE_KEYS = [metric+\"_initial\", metric +\"_ws\"]\n",
    "            for AL_SAMPLING_STRATEGY in [\"UncertaintyMaxMargin_no_ws\",\n",
    "                    \"UncertaintyMaxMargin_with_ws\",\n",
    "                    \"Random\",\n",
    "                    \"CoveredByLeastAmountOfLf\",\n",
    "                    \"ClassificationIsMostWrong\",\n",
    "                    \"GreatestDisagreement\"]:\n",
    "                    COMPARE_KEYS.append(metric + '_ws_and_al_'+AL_SAMPLING_STRATEGY)\n",
    "                    COMPARE_KEYS.append(metric + '_al_and_al_'+AL_SAMPLING_STRATEGY)\n",
    "            #comparison_table(df, COMPARE_KEYS=COMPARE_KEYS, TITLE=\"General Improvement after AL + WS \" + metric)\n",
    "            answer_questions(df[df[group_by] == group_by_value], COMPARE_KEYS=COMPARE_KEYS, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  city keep1  f1_ws_and_al_U  f1_al_and_al_U  f1_ws_and_al_R  f1_al_and_al_R  \\\n",
      "0    A     D               1               4               7              10   \n",
      "1    B     E               2               5               8              11   \n",
      "2    C     F               3               6               9              12   \n",
      "\n",
      "   f1_ws_and_al_X  f1_al_and_al_X  acc_ws_and_al_U  acc_al_and_al_U  \\\n",
      "0              13              16               19               22   \n",
      "1              14              17               20               23   \n",
      "2              15              18               21               24   \n",
      "\n",
      "   acc_ws_and_al_R  acc_al_and_al_R  acc_ws_and_al_X  acc_al_and_al_X  \n",
      "0               25               27               30               33  \n",
      "1               26               28               31               34  \n",
      "2               27               29               32               35  \n",
      "['f1_ws_and_al_U', 'f1_ws_and_al_R', 'f1_ws_and_al_X', 'f1_al_and_al_U', 'f1_al_and_al_R', 'f1_al_and_al_X', 'acc_ws_and_al_U', 'acc_ws_and_al_R', 'acc_ws_and_al_X', 'acc_al_and_al_U', 'acc_al_and_al_R', 'acc_al_and_al_X']\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acc_al_and_al</th>\n      <th>acc_al_and_ws_and_al</th>\n      <th>city</th>\n      <th>f1_al_and_al</th>\n      <th>f1_al_and_ws_and_al</th>\n      <th>keep1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22.0</td>\n      <td>19.0</td>\n      <td>A</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>27.0</td>\n      <td>25.0</td>\n      <td>A</td>\n      <td>10.0</td>\n      <td>7.0</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>33.0</td>\n      <td>30.0</td>\n      <td>A</td>\n      <td>16.0</td>\n      <td>13.0</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>23.0</td>\n      <td>20.0</td>\n      <td>B</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>E</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>28.0</td>\n      <td>26.0</td>\n      <td>B</td>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>E</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34.0</td>\n      <td>31.0</td>\n      <td>B</td>\n      <td>17.0</td>\n      <td>14.0</td>\n      <td>E</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24.0</td>\n      <td>21.0</td>\n      <td>C</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>29.0</td>\n      <td>27.0</td>\n      <td>C</td>\n      <td>12.0</td>\n      <td>9.0</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>35.0</td>\n      <td>32.0</td>\n      <td>C</td>\n      <td>18.0</td>\n      <td>15.0</td>\n      <td>F</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   acc_al_and_al  acc_al_and_ws_and_al city  f1_al_and_al  \\\n0           22.0                  19.0    A           4.0   \n0           27.0                  25.0    A          10.0   \n0           33.0                  30.0    A          16.0   \n1           23.0                  20.0    B           5.0   \n1           28.0                  26.0    B          11.0   \n1           34.0                  31.0    B          17.0   \n2           24.0                  21.0    C           6.0   \n2           29.0                  27.0    C          12.0   \n2           35.0                  32.0    C          18.0   \n\n   f1_al_and_ws_and_al keep1  \n0                  1.0     D  \n0                  7.0     D  \n0                 13.0     D  \n1                  2.0     E  \n1                  8.0     E  \n1                 14.0     E  \n2                  3.0     F  \n2                  9.0     F  \n2                 15.0     F  "
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#andersherum an die Sache gehen : im großen df schauen wo al_ws_al better than al+al -> und dann schauen wie viel prozent davon jeweils auf die einzelnen werte von datasets, … verteilt sind!!\n",
    "df1 = pd.DataFrame({'city':['A','B','C'],\n",
    "                    'keep1':['D', 'E', 'F'],\n",
    "                    'f1_ws_and_al_U':[1,2,3],\n",
    "                    'f1_al_and_al_U':[4,5,6],\n",
    "                    'f1_ws_and_al_R':[7,8,9],\n",
    "                    'f1_al_and_al_R':[10,11,12],\n",
    "                    'f1_ws_and_al_X':[13,14,15],\n",
    "                    'f1_al_and_al_X':[16,17,18],\n",
    "\n",
    "                    'acc_ws_and_al_U':[19,20,21],\n",
    "                    'acc_al_and_al_U':[22,23,24],\n",
    "                    'acc_ws_and_al_R':[25,26,27],\n",
    "                    'acc_al_and_al_R':[27,28,29],\n",
    "                    'acc_ws_and_al_X':[30,31,32],\n",
    "                    'acc_al_and_al_X':[33,34,35],\n",
    "})\n",
    "print(df1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "al_sampling_strategies = [\"U\", \"R\", \"X\"]\n",
    "to_split_metrics = (\n",
    "    [\"f1_ws_and_al_\" + k for k in al_sampling_strategies]\n",
    "    + [\"f1_al_and_al_\" + k for k in al_sampling_strategies]\n",
    "    + [\"acc_ws_and_al_\" + k for k in al_sampling_strategies]\n",
    "    + [\"acc_al_and_al_\" + k for k in al_sampling_strategies]\n",
    ")\n",
    "print(to_split_metrics)\n",
    "\n",
    "for _, row in df1.iterrows():\n",
    "    for al_sampling_strategy in al_sampling_strategies:\n",
    "        # remove all other metrics\n",
    "        row2 = row.drop(\n",
    "            [m for m in to_split_metrics if not m.endswith(al_sampling_strategy)]\n",
    "        )\n",
    "        row2 = row2.rename(\n",
    "            {\n",
    "                \"f1_ws_and_al_\" + al_sampling_strategy: \"f1_al_and_ws_and_al\",\n",
    "                \"acc_ws_and_al_\" + al_sampling_strategy: \"acc_al_and_ws_and_al\",\n",
    "                \"f1_al_and_al_\" + al_sampling_strategy: \"f1_al_and_al\",\n",
    "                \"acc_al_and_al_\" + al_sampling_strategy: \"acc_al_and_al\",\n",
    "            }\n",
    "        )\n",
    "        df_new = df_new.append(row2)\n",
    "\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f1_ws_and_al_UncertaintyMaxMargin_no_ws', 'f1_ws_and_al_UncertaintyMaxMargin_with_ws', 'f1_ws_and_al_Random', 'f1_ws_and_al_CoveredByLeastAmountOfLf', 'f1_ws_and_al_ClassificationIsMostWrong', 'f1_ws_and_al_GreatestDisagreement', 'f1_al_and_al_UncertaintyMaxMargin_no_ws', 'f1_al_and_al_UncertaintyMaxMargin_with_ws', 'f1_al_and_al_Random', 'f1_al_and_al_CoveredByLeastAmountOfLf', 'f1_al_and_al_ClassificationIsMostWrong', 'f1_al_and_al_GreatestDisagreement', 'acc_ws_and_al_UncertaintyMaxMargin_no_ws', 'acc_ws_and_al_UncertaintyMaxMargin_with_ws', 'acc_ws_and_al_Random', 'acc_ws_and_al_CoveredByLeastAmountOfLf', 'acc_ws_and_al_ClassificationIsMostWrong', 'acc_ws_and_al_GreatestDisagreement', 'acc_al_and_al_UncertaintyMaxMargin_no_ws', 'acc_al_and_al_UncertaintyMaxMargin_with_ws', 'acc_al_and_al_Random', 'acc_al_and_al_CoveredByLeastAmountOfLf', 'acc_al_and_al_ClassificationIsMostWrong', 'acc_al_and_al_GreatestDisagreement']\n"
     ]
    }
   ],
   "source": [
    "CREATE_CSV = True\n",
    "if CREATE_CSV:\n",
    "    # unravel al sampling strat metrics\n",
    "    al_sampling_strategies = [\n",
    "        \"UncertaintyMaxMargin_no_ws\",\n",
    "        \"UncertaintyMaxMargin_with_ws\",\n",
    "        \"Random\",\n",
    "        \"CoveredByLeastAmountOfLf\",\n",
    "        \"ClassificationIsMostWrong\",\n",
    "        \"GreatestDisagreement\",\n",
    "    ]\n",
    "\n",
    "    df_new = pd.DataFrame()\n",
    "    to_split_metrics = (\n",
    "        [\"f1_ws_and_al_\" + k for k in al_sampling_strategies]\n",
    "        + [\"f1_al_and_al_\" + k for k in al_sampling_strategies]\n",
    "        + [\"acc_ws_and_al_\" + k for k in al_sampling_strategies]\n",
    "        + [\"acc_al_and_al_\" + k for k in al_sampling_strategies]\n",
    "    )\n",
    "    print(to_split_metrics)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        for al_sampling_strategy in al_sampling_strategies:\n",
    "            # remove all other metrics\n",
    "            row2 = row.drop(\n",
    "                [m for m in to_split_metrics if not m.endswith(al_sampling_strategy)]\n",
    "            )\n",
    "            row2 = row2.rename(\n",
    "                {\n",
    "                    \"f1_ws_and_al_\" + al_sampling_strategy: \"f1_al_and_ws_and_al\",\n",
    "                    \"acc_ws_and_al_\" + al_sampling_strategy: \"acc_al_and_ws_and_al\",\n",
    "                    \"f1_al_and_al_\" + al_sampling_strategy: \"f1_al_and_al\",\n",
    "                    \"acc_al_and_al_\" + al_sampling_strategy: \"acc_al_and_al\",\n",
    "                }\n",
    "            )\n",
    "            row2['al_sampling_strategy'] = al_sampling_strategy\n",
    "            df_new = df_new.append(row2)\n",
    "\n",
    "    df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_CSV:\n",
    "    df_new.to_csv(\"how_to.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-149-dd7fda190f1f>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-149-dd7fda190f1f>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    hist_keys_that_are_lists =  [\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"how_to.csv\")\n",
    "\n",
    "print(df.keys())\n",
    "print(\"Total: \", len(df))\n",
    "\n",
    "no_hist_keys = [\n",
    "    \"ABSTAIN_THRESHOLDS\",\n",
    "    \"AL_SAMPLES_WEIGTHS\",\n",
    "    \"DATASET_RANDOM_GENERATION_SEED\",\n",
    "    \"JOB_ID\",\n",
    "    \"LF_CLASSIFIERS\",\n",
    "    \"LF_RANDOM_SEED\",\n",
    "    \"random_state\",\n",
    "    \"AMOUNT_OF_LF_FEATURESSS\",\n",
    "\n",
    "\n",
    "hist_keys_that_are_lists =  [\n",
    "    \"ABSTAIN_THRESHOLDS\",\n",
    "    \"LF_CLASSIFIERS\",\n",
    "    \"AMOUNT_OF_LF_FEATURESSS\",\n",
    "]\n",
    "\n",
    "# 1. check the value ranges -> does it make sense to go one_hot_encoding? for LF_CLASSIFIERS definitely\n",
    "# 2. for the rest not so much -,-\n",
    "\n",
    "\n",
    "title=\"all\"\n",
    "for key in df:\n",
    "    if key.startswith(\"acc_\") or key.startswith(\"f1_\") or key in no_hist_keys:\n",
    "        continue\n",
    "    print(key)\n",
    "    sns.histplot(df, y=key).set(title=title)\n",
    "    plt.savefig('plots/' + title+ '_' + key + '.jpg', dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig('plots/' + title+ '_' + key + '.pdf',dpi=300, format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def analyse_value_distributions(df, df_a, df_not_a, title):\n",
    "    for key in df_a:\n",
    "        if key.startswith(\"acc_\") or key.startswith(\"f1_\") or key in no_hist_keys:\n",
    "            continue\n",
    "        print(key)\n",
    "\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, sharex=True)\n",
    "        ax1.set_title(title)\n",
    "        sns.histplot(df_a, y=key, ax=ax1)\n",
    "        sns.histplot(df_not_a, y=key, ax=ax2)\n",
    "        plt.savefig(\"plots/\" + title + \"_\" + key + \".jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.savefig(\n",
    "            \"plots/\" + title + \"_\" + key + \".pdf\",\n",
    "            dpi=300,\n",
    "            format=\"pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.clf()\n",
    "        #acc per lf als stärke der lf\n",
    "\n",
    "for metric in [\"acc\", \"f1\"]:\n",
    "    print(metric)\n",
    "        \n",
    "    analyse_value_distributions(\n",
    "        df,\n",
    "        df.loc[(df[metric + \"_initial\"] > df[metric + \"_al_and_al\"])\n",
    "            & (df[metric + \"_initial\"] > df[metric + \"_al_and_ws_and_al\"])],\n",
    "        df.loc[(df[metric + \"_initial\"] <= df[metric + \"_al_and_al\"])\n",
    "            & (df[metric + \"_initial\"] <= df[metric + \"_al_and_ws_and_al\"])],\n",
    "        metric + \" Initial was better than everything else\",\n",
    "    )\n",
    "    analyse_value_distributions(\n",
    "        df,\n",
    "        df.loc[(df[metric + \"_ws\"] > df[metric + \"_initial\"])],\n",
    "        df.loc[(df[metric + \"_ws\"] >= df[metric + \"_initial\"])],\n",
    "         metric + \"Only WS > Initial\",\n",
    "    )\n",
    "    analyse_value_distributions(\n",
    "        df,\n",
    "        df.loc[(df[metric + \"_al_and_ws_and_al\"] > df[metric + \"_ws\"])],\n",
    "        df.loc[(df[metric + \"_al_and_ws_and_al\"] <= df[metric + \"_ws\"])],\n",
    "         metric + \" AL and WS and AL > Only WS\",\n",
    "    )\n",
    "    analyse_value_distributions(\n",
    "        df,\n",
    "        df.loc[ (df[metric + \"_al_and_al\"] > df[metric + \"_initial\"])],\n",
    "        df.loc[ (df[metric + \"_al_and_al\"] <= df[metric + \"_initial\"])],\n",
    "         metric + \" AL and AL > AL\",\n",
    "    )\n",
    "    analyse_value_distributions(\n",
    "        df,\n",
    "        df.loc[(df[metric + \"_al_and_ws_and_al\"] < df[metric + \"_ws\"])],\n",
    "        df.loc[(df[metric + \"_al_and_ws_and_al\"] >= df[metric + \"_ws\"])],\n",
    "        metric + \" AL and WS and AL < Only WS\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABSTAIN_THRESHOLDS': defaultdict(<function <lambda> at 0x7fb2351a0dc0>, {0.0: 13206, 0.9: 27030, 0.5: 26460, 0.4: 26622, 0.1: 26292, 0.2: 26922, 1.0: 13578, 0.7: 26652, 0.6: 26328, 0.3: 26382, 0.8: 26286}), 'AL_SAMPLES_WEIGHT': defaultdict(<function <lambda> at 0x7fb2371c8550>, {30: 5130, 10: 4698, 50: 5070, 40: 6282, 70: 5010, 0: 3138, 80: 6354, 90: 4932, 60: 5910, 100: 2862, 20: 6648}), 'LF_CLASSIFIERS': defaultdict(<function <lambda> at 0x7fb2371c81f0>, {'knn': 88164, 'dt': 89010, 'lr': 88584}), 'AMOUNT_OF_LF_FEATURESSS': defaultdict(<function <lambda> at 0x7fb22da9e9d0>, {1: 161040, 2: 46206, 5: 13818, 3: 26574, 4: 18120})}\n",
      "ABSTAIN_THRESHOLDS\n",
      "AL_SAMPLES_WEIGHT\n",
      "LF_CLASSIFIERS\n",
      "AMOUNT_OF_LF_FEATURESSS\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from collections import defaultdict\n",
    "\n",
    "df = pd.read_csv(\"how_to.csv\")\n",
    "\n",
    "hist_keys_that_are_lists =  [\n",
    "    \"ABSTAIN_THRESHOLDS\", # first binarize, then count values?\n",
    "    \"LF_CLASSIFIERS\", # -> #dt #knn #lr -> count values\n",
    "    \"AMOUNT_OF_LF_FEATURESSS\", # -> #1lfs #2lfs #3fs -> count values\n",
    "]\n",
    "key_values = {}\n",
    "for key in hist_keys_that_are_lists:\n",
    "    key_values[key] = defaultdict(lambda: 0)\n",
    "\n",
    "    for row in df[key]:\n",
    "        if key == 'AL_SAMPLES_WEIGHT':\n",
    "            parsed_object = int(row)\n",
    "        else:\n",
    "            parsed_object = ast.literal_eval(row)\n",
    "\n",
    "        if isinstance(parsed_object, list):\n",
    "            for value in parsed_object:\n",
    "                if key != 'ABSTAIN_THRESHOLDS':\n",
    "                    key_values[key][value] += 1 \n",
    "                else:\n",
    "                    key_values[key][round(value,1)] += 1\n",
    "        else:\n",
    "            key_values[key][10*round(parsed_object/10)] += 1\n",
    "    #print(key)\n",
    "    #print(key_values)\n",
    "print(key_values)\n",
    "CREATE_CSV = True\n",
    "if CREATE_CSV:\n",
    "    # initialize new colums with 0\n",
    "    for k,v in key_values.items():\n",
    "        for value in v.keys():\n",
    "            df[k+'_'+str(value)] = 0\n",
    "\n",
    "    for key in hist_keys_that_are_lists:\n",
    "        print(key)\n",
    "        for ix, row in df[key].iteritems():\n",
    "\n",
    "            if key == 'AL_SAMPLES_WEIGHT':\n",
    "                parsed_object = int(row)\n",
    "            else:\n",
    "                parsed_object = ast.literal_eval(row)\n",
    "                \n",
    "            if isinstance(parsed_object, list):\n",
    "                for value in parsed_object:\n",
    "                    if key != 'ABSTAIN_THRESHOLDS':\n",
    "                        df.loc[ix, key + '_' + str(value)] += 1\n",
    "                    else:\n",
    "                        df.loc[ix, key + '_' + str(round(value,1))] += 1\n",
    "            else:\n",
    "                df.loc[ix, key + '_' + str(10*round(parsed_object/10))] += 1\n",
    "\n",
    "    df.drop(hist_keys_that_are_lists, axis=1, inplace=True)\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-794c66d7d4dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv('new.csv', index=False)\n",
    "df.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('code': pipenv)",
   "name": "python3810jvsc74a57bd0178c561f06d1c9b562d26393e106d1bb838dc5f57458805df561d13c135e6443"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}