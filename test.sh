#!/bin/bash

# for i in {33..100}
# do
#     python imit_training.py --DATASETS_PATH ../datasets --OUTPUT_DIRECTORY opti --CLUSTER dummy --NR_QUERIES_PER_ITERATION 1 --DATASET_NAME synthetic --START_SET_SIZE 1 --USER_QUERY_BUDGET_LIMIT 20 --RANDOM_SEED $i --N_JOBS 8 --AMOUNT_OF_PEAKED_OBJECTS 20 --MAX_AMOUNT_OF_WS_PEAKS 0 --AMOUNT_LEARN_ITERATIONS 1 --PLOT_EVOLUTION --AMOUNT_OF_FEATURES 2 --CONVEX_HULL_SAMPLING --VARIABLE_INPUT_SIZE --NEW_SYNTHETIC_PARAMS --HYPERCUBE
# done
# 

for i in {1..100}
do
    python single_al_cycle.py --OUTPUT_DIRECTORY trained --NN_BINARY tmp/_vd_true_100_rf_true_-1_h_false_nsp_true_chs_true_2_5/trained_ann.pickle --SAMPLING trained_nn --CLUSTER dummy --NR_QUERIES_PER_ITERATION 1 --DATASET_NAME synthetic --START_SET_SIZE 1 --USER_QUERY_BUDGET_LIMIT 20 --RANDOM_SEED $i --N_JOBS 8 --PLOT_EVOLUTION --AMOUNT_OF_FEATURES 2 --NR_LEARNING_ITERATIONS 20 --CONVEX_HULL_SAMPLING --VARIABLE_INPUT_SIZE --NEW_SYNTHETIC_PARAMS --HYPERCUBE --REPRESENTATIVE_FEATURES
done
 
for i in {1..100}
do
    python single_al_cycle.py --OUTPUT_DIRECTORY trained_new  --NN_BINARY tmp/_vd_true_100_rf_true_-1_h_false_nsp_true_chs_true_2_5/trained_ann.pickle --SAMPLING trained_nn --CLUSTER dummy --NR_QUERIES_PER_ITERATION 1 --DATASET_NAME synthetic --START_SET_SIZE 1 --USER_QUERY_BUDGET_LIMIT 20 --RANDOM_SEED $i --N_JOBS 8 --PLOT_EVOLUTION --AMOUNT_OF_FEATURES 2 --NR_LEARNING_ITERATIONS 20 --CONVEX_HULL_SAMPLING --VARIABLE_INPUT_SIZE --HYPERCUBE --REPRESENTATIVE_FEATURES
done


# 
# for i in {1..100}
# do
#     python single_al_cycle.py --OUTPUT_DIRECTORY random --SAMPLING random --CLUSTER dummy --NR_QUERIES_PER_ITERATION 1 --DATASET_NAME synthetic --START_SET_SIZE 1 --USER_QUERY_BUDGET_LIMIT 20 --RANDOM_SEED $i --N_JOBS 8 --PLOT_EVOLUTION --AMOUNT_OF_FEATURES 2 --NR_LEARNING_ITERATIONS 20 --CONVEX_HULL_SAMPLING --VARIABLE_INPUT_SIZE --NEW_SYNTHETIC_PARAMS --HYPERCUBE
# done
# 
# 
# for i in {1..100}
# do
#     python single_al_cycle.py --OUTPUT_DIRECTORY unc --SAMPLING uncertainty_max_margin --CLUSTER dummy --NR_QUERIES_PER_ITERATION 1 --DATASET_NAME synthetic --START_SET_SIZE 1 --USER_QUERY_BUDGET_LIMIT 20 --RANDOM_SEED $i --N_JOBS 8 --PLOT_EVOLUTION --AMOUNT_OF_FEATURES 2 --NR_LEARNING_ITERATIONS 20 --CONVEX_HULL_SAMPLING --VARIABLE_INPUT_SIZE --NEW_SYNTHETIC_PARAMS --HYPERCUBE
# done
# 
# 
# for i in {1..100}
# do
#     python imit_training.py --DATASETS_PATH ../datasets --OUTPUT_DIRECTORY opti_new --CLUSTER dummy --NR_QUERIES_PER_ITERATION 1 --DATASET_NAME synthetic --START_SET_SIZE 1 --USER_QUERY_BUDGET_LIMIT 20 --RANDOM_SEED $i --N_JOBS 8 --AMOUNT_OF_PEAKED_OBJECTS 20 --MAX_AMOUNT_OF_WS_PEAKS 0 --NR_LEARNING_ITERATIONS 20 --PLOT_EVOLUTION --AMOUNT_OF_FEATURES 2 --CONVEX_HULL_SAMPLING --VARIABLE_INPUT_SIZE --HYPERCUBE
# done
# 
# 
# for i in {1..100}
# do
#     python single_al_cycle.py --OUTPUT_DIRECTORY random_new --SAMPLING random --CLUSTER dummy --NR_QUERIES_PER_ITERATION 1 --DATASET_NAME synthetic --START_SET_SIZE 1 --USER_QUERY_BUDGET_LIMIT 20 --RANDOM_SEED $i --N_JOBS 8 --PLOT_EVOLUTION --AMOUNT_OF_FEATURES 2 --NR_LEARNING_ITERATIONS 20 --CONVEX_HULL_SAMPLING --VARIABLE_INPUT_SIZE --HYPERCUBE
# done
# 
# 
# for i in {1..100}
# do
#     python single_al_cycle.py --OUTPUT_DIRECTORY unc_new --SAMPLING uncertainty_max_margin --CLUSTER dummy --NR_QUERIES_PER_ITERATION 1 --DATASET_NAME synthetic --START_SET_SIZE 1 --USER_QUERY_BUDGET_LIMIT 20 --RANDOM_SEED $i --N_JOBS 8 --PLOT_EVOLUTION --AMOUNT_OF_FEATURES 2 --NR_LEARNING_ITERATIONS 20 --CONVEX_HULL_SAMPLING --VARIABLE_INPUT_SIZE --HYPERCUB
# done
# 
# 
